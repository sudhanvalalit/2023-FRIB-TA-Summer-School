{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PESNet\n",
    "\n",
    "This notebook contains a schematic neural network architecture for producing potential energy surfaces for given nucleus. The input to the example network is [A,Z,Q20,Q30], with the output being the energy. \n",
    "\n",
    "The philosophy behind this structure is to produce a network that can quickly produce full surfaces for nuclei, perhaps as a guiding light for a HFB calculation. The network isn't expected to work well for extrapolation beyond the training set of nuclei, where the PES may have exotic behavior.\n",
    "\n",
    "A secondary method of training is to actively exclude nuclei to benchmark the performance on whole surfaces. This can be done for a few nuclei, though one should probably disable the test split if you're excluding several whole nuclei.\n",
    "\n",
    "An alternate method (and thus a good one for a final project!) to produce surfaces would be to train a network that, instead, takes a few points as input (say 5%-10% of a surface) to then supersample the rest of the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as skl\n",
    "import sklearn as sk\n",
    "from pylab import plt, mpl\n",
    "from pickle import dump, load\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler,RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF for faster training\")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "def MakePlot(x,y, styles, labels, axlabels):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(len(x)):\n",
    "        plt.plot(x[i], y[i], styles[i], label = labels[i])\n",
    "        plt.xlabel(axlabels[0])\n",
    "        plt.ylabel(axlabels[1])\n",
    "    plt.legend(loc=0)\n",
    "    \n",
    "# R2 metric for tensorflow from https://jmlb.github.io/ml/2017/03/20/CoeffDetermination_CustomMetric4Keras/\n",
    "def R2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def PESpredict(X, model, scaler):\n",
    "    X = np.asarray(X)\n",
    "    X = X.T\n",
    "    xx = scaler.transform(X)\n",
    "    pes=model.predict(xx)\n",
    "    \n",
    "    return pes.T.squeeze()\n",
    "\n",
    "\n",
    "def NucError(A,Z,df,model,scaler):\n",
    "    PES_sub = df[(df['A'] == A) & (df['Z'] == Z)]\n",
    "\n",
    "    print(\"Nucleus: A =\",int(A),\" Z =\",int(Z))\n",
    "\n",
    "    A = PES_sub['A']\n",
    "    Z = PES_sub['Z']\n",
    "    Q20 = PES_sub['Q20']\n",
    "    Q30 = PES_sub['Q30']\n",
    "    E = PES_sub['HFB_cubic']\n",
    "\n",
    "    xx = (A.to_numpy(),Z.to_numpy(),Q20.to_numpy(),Q30.to_numpy())\n",
    "\n",
    "    pes = PESpredict(xx, model, scaler)\n",
    "\n",
    "    print(\"Chosen nucleus error: \",sum((pes - np.asarray(E))**2)/pes.size)\n",
    "    \n",
    "def PlotSurface(A,Z,df,model,scaler):\n",
    "    PES_sub = df[(df['A'] == A) & (df['Z'] == Z)]\n",
    "\n",
    "    A = PES_sub['A']\n",
    "    Z = PES_sub['Z']\n",
    "    Q20 = PES_sub['Q20']\n",
    "    Q30 = PES_sub['Q30']\n",
    "    E = PES_sub['HFB_cubic']\n",
    "\n",
    "    xx = (A.to_numpy(),Z.to_numpy(),Q20.to_numpy(),Q30.to_numpy())\n",
    "\n",
    "    pes = PESpredict(xx, model, scaler)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,2,figsize=(20,8), gridspec_kw={'width_ratios': [0.8, 1]})\n",
    "\n",
    "    # Plot the surface.\n",
    "    cmp = 'coolwarm'\n",
    "\n",
    "    surf = axs[0].contour(np.reshape(Q20.to_numpy(),(126,21)), np.reshape(Q30.to_numpy(),(126,21)), np.reshape(E.to_numpy(),(126,21)))#, cmap=mpl.cm.seismic)\n",
    "    axs[0].set_title(\"From Data\")\n",
    "    img = axs[0].imshow(np.reshape(E.to_numpy(),(126,21)).T, extent=[0, 250, 0, 60], origin='lower',\n",
    "           cmap=cmp,interpolation='bilinear',aspect=\"auto\")\n",
    "    surf = axs[1].contour(np.reshape(Q20.to_numpy(),(126,21)), np.reshape(Q30.to_numpy(),(126,21)), np.reshape(pes,(126,21)))#, cmap=mpl.cm.seismic)\n",
    "    axs[1].set_title(\"From NN\")\n",
    "    img = axs[1].imshow(np.reshape(pes,(126,21)).T, extent=[0, 250, 0, 60], origin='lower',\n",
    "           cmap=cmp,interpolation='bilinear',aspect=\"auto\")\n",
    "    fig.colorbar(img)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"Data/\"\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell can be ran once you have a trained model and saved scaler to spot check the performance of the network for certain nuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_check():\n",
    "    model = tf.keras.models.load_model('saved_model/PESNet.model',custom_objects={'R2':R2})\n",
    "    xscaler = load( open( 'pickles/xscaler.pkl', \"rb\" ) )\n",
    "\n",
    "    infile = open(data_path(\"All.dat\"),'r')\n",
    "\n",
    "    PESfull = pd.read_csv(infile,delim_whitespace=True,low_memory=False)\n",
    "\n",
    "    NucError(304,100,PESfull,model,xscaler)\n",
    "    NucError(232,88,PESfull,model,xscaler)\n",
    "    NucError(264,94,PESfull,model,xscaler)\n",
    "    NucError(282,94,PESfull,model,xscaler)\n",
    "    NucError(360,110,PESfull,model,xscaler)\n",
    "    NucError(238,94,PESfull,model,xscaler)\n",
    "    NucError(256,98,PESfull,model,xscaler)\n",
    "    NucError(324,108,PESfull,model,xscaler)\n",
    "\n",
    "    PlotSurface(304,100,PESfull,model,xscaler)\n",
    "    PlotSurface(232,88,PESfull,model,xscaler)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling our data\n",
    "\n",
    "First, let's load in the data. I've shoved all the potential energy surfaces into a file called `All.dat`, so we'll load that into a dataframe and get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infile = open(data_path(\"All.dat\"),'r')\n",
    "\n",
    "PES = pd.read_csv(infile,delim_whitespace=True,low_memory=False)\n",
    "\n",
    "A = PES['A']\n",
    "Z = PES['Z']\n",
    "Q20 = PES['Q20']\n",
    "Q30 = PES['Q30']\n",
    "E = PES['HFB_cubic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will put together the input numpy arrays for tensorflow. I'll also do the train/test splitting here as well as some scaling -- this would be a good place to explore changing things around to squeeze some additional performance out of your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input array\n",
    "xx = (A.to_numpy(),Z.to_numpy(),Q20.to_numpy(),Q30.to_numpy())\n",
    "xx = np.asarray(xx)\n",
    "xx = xx.T\n",
    "\n",
    "yy = np.asarray(E).reshape(-1, 1)\n",
    "\n",
    "# Get a test set for later\n",
    "x_train, x_test, y_train, y_test = train_test_split(xx,yy,test_size=0.25,random_state=42)\n",
    "x_train,y_train = sk.utils.shuffle(x_train,y_train, random_state=42)\n",
    "\n",
    "# Scale input\n",
    "xscaler = RobustScaler(unit_variance=True)# Try out MinMaxScaler() or other ones!\n",
    "xscaler.fit(xx)\n",
    "\n",
    "xs_train = xscaler.transform(x_train)\n",
    "xs_test = xscaler.transform(x_test)\n",
    "xs_full = xscaler.transform(xx)\n",
    "\n",
    "# Scale output with the following lines.\n",
    "# I don't scale anything, but give it a try and see how it affects your results!\n",
    "\n",
    "#yscaler = MinMaxScaler()\n",
    "#yscaler.fit(y_train)\n",
    "\n",
    "ys_train = y_train #yscaler.transform(y_train)\n",
    "ys_test = y_test #yscaler.transform(y_test)\n",
    "ys_full = yy #yscaler.transform(yy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network\n",
    "\n",
    "Now we actually build our neural network! I have some skeleton code for the input and here, but I don't want to bias you too much -- play with keras layer types, node configurations, activation functions, input information, etc. \n",
    "\n",
    "For information on the keras API, check the documentation: https://keras.io/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = # This is a handy helper variable for programmatic node counts\n",
    "activation = # I recommend you pick something nice from tf.keras.layers\n",
    "\n",
    "model=tf.keras.Sequential() #Define the model object\n",
    "\n",
    "# Input layer assuming we pass in A, Z, Q20, Q30\n",
    "model.add(tf.keras.layers.Dense(nodes,input_shape=(4,),activation=activation))\n",
    "\n",
    "# Hidden layers\n",
    "model.add()\n",
    "\n",
    "\n",
    "# Here's an output assuming you just want to output the energy\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss='mean_squared_error',metrics=[R2]) #Adam optimizer and mean squared error loss\n",
    "\n",
    "# Try different optimizers too! It will help you get a feel for what each one actually does\n",
    "\n",
    "#model.compile(tf.keras.optimizers.Adadelta(),loss='mean_squared_error',metrics=[R2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model and checking the performance\n",
    "\n",
    "With our bespoke neural network in hand, let's train! I have some reasonable defaults here, but try new things! Break the optimizer! Crash your computer! The possibilities are endless.\n",
    "\n",
    "I also have some handy plotting features and loss computation down below, but by all means plot different quantities to see if they reveal something intriguing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=model.fit(xs_train,ys_train,epochs=100, batch_size=512, validation_split=0.25,verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = results.history\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "plt.plot(history[\"loss\"], label=\"training loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "ax.set_yscale(\"log\", nonpositive='clip')\n",
    "#plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loss calculation\n",
    "\n",
    "#[test_loss,test_R2]=model.evaluate(xs_test, ys_test, verbose=1)\n",
    "\n",
    "#print('Test Loss: {:.04}'.format(test_loss))\n",
    "#print('Test R2: {:.04}'.format(test_R2))\n",
    "\n",
    "testen_out=model.predict(xs_test)\n",
    "trainen_out=model.predict(xs_train)\n",
    "fullen_out=model.predict(xs_full)\n",
    "\n",
    "# Shift back for plotting\n",
    "\n",
    "x_train = xscaler.inverse_transform(xs_train)\n",
    "x_test = xscaler.inverse_transform(xs_test)\n",
    "x_full = xscaler.inverse_transform(xs_full)\n",
    "\n",
    "trainen_out = trainen_out.T.squeeze()#yscaler.inverse_transform(trainen_out).T.squeeze()\n",
    "testen_out = testen_out.T.squeeze()#yscaler.inverse_transform(testen_out).T.squeeze()\n",
    "fullen_out = fullen_out.T.squeeze()#yscaler.inverse_transform(fullen_out).T.squeeze()\n",
    "\n",
    "a_test=np.sum(x_test,axis=1)\n",
    "a_train=np.sum(x_train,axis=1)\n",
    "a_full=np.sum(x_full,axis=1)\n",
    "\n",
    "en_test=np.asarray(y_test).squeeze()\n",
    "trainen_test=np.asarray(y_train).squeeze()\n",
    "fullen_test=np.asarray(yy).squeeze()\n",
    "\n",
    "diff = en_test-testen_out\n",
    "traindiff = trainen_test-trainen_out\n",
    "fulldiff = fullen_test-fullen_out\n",
    "\n",
    "diff = np.asarray(diff.T).squeeze()\n",
    "fulldiff = np.asarray(fulldiff.T).squeeze()\n",
    "traindiff = np.asarray(traindiff.T).squeeze()\n",
    "\n",
    "x_test_even = x_test[(np.rint(x_test[:,0])%2==0) & (np.rint(x_test[:,1])%2==0)]\n",
    "diff_even = diff[(np.rint(x_test[:,0])%2==0) & (np.rint(x_test[:,1])%2==0)]\n",
    "x_train_even = x_train[(np.rint(x_train[:,0])%2==0) & (np.rint(x_train[:,1])%2==0)]\n",
    "traindiff_even = traindiff[(np.rint(x_train[:,0])%2==0) & (np.rint(x_train[:,1])%2==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell in particular computes and plots the results for a chosen nucleus. I recommend doing this sort of investigation if you're trying to improve performance in certain regions of the nuclear chart and have some standard benchmark to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse = model\n",
    "\n",
    "#PES = PESfull[ (((PESfull['A'] != 304) & (PESfull['Z'] != 100)))]\n",
    "PESfull = PES\n",
    "A=360.0\n",
    "Z=110.0\n",
    "\n",
    "PES_sub = PESfull[(PESfull['A'] == A) & (PESfull['Z'] == Z)]\n",
    "\n",
    "print(\"Nucleus: A =\",int(A),\" Z =\",int(Z))\n",
    "\n",
    "A = PES_sub['A']\n",
    "Z = PES_sub['Z']\n",
    "Q20 = PES_sub['Q20']\n",
    "Q30 = PES_sub['Q30']\n",
    "E = PES_sub['HFB_cubic']\n",
    "\n",
    "xx = (A.to_numpy(),Z.to_numpy(),Q20.to_numpy(),Q30.to_numpy())\n",
    "xx = np.asarray(xx)\n",
    "xx = xx.T\n",
    "\n",
    "xs_full = xscaler.transform(xx)\n",
    "\n",
    "yy = np.asarray(E).reshape(-1, 1)\n",
    "\n",
    "#ys_full = yscaler.transform(yy)\n",
    "\n",
    "#model.evaluate(xs_test, ys_test, verbose=1)\n",
    "pes=model_mse.predict(xs_full)\n",
    "\n",
    "# Shift back for plotting\n",
    "\n",
    "x_full = xscaler.inverse_transform(xs_full)\n",
    "\n",
    "fullen_out = pes.T.squeeze()#yscaler.inverse_transform(pes).T.squeeze()\n",
    "\n",
    "print(\"Chosen nucleus error: \",sum((fullen_out - np.asarray(E))**2)/fullen_out.size)\n",
    "\n",
    "#print(traindiff)\n",
    "t2 = traindiff**2\n",
    "print(\"Training error: \",sum(t2)/traindiff.size)\n",
    "\n",
    "t2 = diff**2\n",
    "print(\"Test set error: \",sum(t2)/diff.size)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(20,8), gridspec_kw={'width_ratios': [0.8, 1]})\n",
    "#im = ax.imshow(np.reshape(Q20.to_numpy(),(126,21)), np.reshape(Q30.to_numpy(),(126,21)), np.reshape(E.to_numpy(),(126,21)), interpolation='bilinear', origin='lower')#,cmap=mpl.cm.seismic)\n",
    "\n",
    "# Plot the surface.\n",
    "\n",
    "cmp = 'coolwarm'\n",
    "\n",
    "surf = axs[0].contour(np.reshape(Q20.to_numpy(),(126,21)), np.reshape(Q30.to_numpy(),(126,21)), np.reshape(E.to_numpy(),(126,21)))#, cmap=mpl.cm.seismic)\n",
    "axs[0].set_title(\"From Data\")\n",
    "img = axs[0].imshow(np.reshape(E.to_numpy(),(126,21)).T, extent=[0, 250, 0, 60], origin='lower',\n",
    "           cmap=cmp,interpolation='bilinear',aspect=\"auto\")\n",
    "surf = axs[1].contour(np.reshape(Q20.to_numpy(),(126,21)), np.reshape(Q30.to_numpy(),(126,21)), np.reshape(fullen_out,(126,21)))#, cmap=mpl.cm.seismic)\n",
    "axs[1].set_title(\"From NN\")\n",
    "img = axs[1].imshow(np.reshape(fullen_out,(126,21)).T, extent=[0, 250, 0, 60], origin='lower',\n",
    "           cmap=cmp,interpolation='bilinear',aspect=\"auto\")\n",
    "fig.colorbar(img)\n",
    "\n",
    "# Customize the z axis.\n",
    "#ax.set_zlim(-1.01, 1.01)\n",
    "\n",
    "#ax.zaxis.set_major_formatter('{x:.02f}')\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "#fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "#cbar = fig.colorbar(surf)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our model\n",
    "\n",
    "The nice thing about a trained neural network is that it can then be deployed relatively easily for the purposes of inference. To enable that, we'll save our model and scaler so we can use it later. If you're making a lot of networks and want to pit them against each other in a battle royale be sure to save them with reasonably informative names. Consider being systematic with your tests as well -- you may be able to draw some interesting conclusions from these investigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/PESNet.model')\n",
    "\n",
    "# Saved model with standard test/training.val splits:\n",
    "# Training error:  XXXX\n",
    "# Test set error:  XXXX\n",
    "\n",
    "!mkdir -p pickles\n",
    "# save the scaler\n",
    "dump(xscaler, open('pickles/xscaler.pkl', 'wb'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying new things!\n",
    "\n",
    "The above methodology is really the simplest/most straightforward way to do this stuff, but there are many many other things that you can try to improve your performance globally or to test the ability of the network to extrapolate to new nuclei it has never seen before.\n",
    "\n",
    "In your remaining time I recommend you start throwing things at the wall and see what sticks! In addition to trying different network architectures like mentioned above, you can also change what gets fed into your network during training and inference. One example would be to feed in the ground state energy for the nucleus from the liquid drop model, for instance. Will it be useful to the network? Who knows!\n",
    "\n",
    "To test your model's performance in extrapolation, you can explicitly exclude certain nuclei's PES from the training data set. This gives you a nice set of benchmarks that should be entirely foreign to the trained model. The following cell shows you how to prepare such a dataset and you can copy it up above or write the rest below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infile = open(data_path(\"All.dat\"),'r')\n",
    "\n",
    "\n",
    "PESfull = pd.read_csv(infile,delim_whitespace=True,low_memory=False)\n",
    "PES = PESfull[~((PESfull['A'] == 304) & (PESfull['Z'] == 100))]\n",
    "PES = PES[~((PES['A'] == 232) & (PES['Z'] == 88))]\n",
    "PES = PES[~((PES['A'] == 264) & (PES['Z'] == 94))]\n",
    "PES = PES[~((PES['A'] == 282) & (PES['Z'] == 94))]\n",
    "PES = PES[~((PES['A'] == 360) & (PES['Z'] == 110))]\n",
    "PES = PES[~((PES['A'] == 238) & (PES['Z'] == 94))]\n",
    "PES = PES[~((PES['A'] == 256) & (PES['Z'] == 98))]\n",
    "PES = PES[~((PES['A'] == 324) & (PES['Z'] == 108))]\n",
    "\n",
    "A = PES['A']\n",
    "Z = PES['Z']\n",
    "Q20 = PES['Q20']\n",
    "Q30 = PES['Q30']\n",
    "E = PES['HFB_cubic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation and plotting for one of the excluded nuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_msle = model \n",
    "\n",
    "A=256.0\n",
    "Z=98.0\n",
    "\n",
    "PES_sub = PESfull[(PESfull['A'] == A) & (PESfull['Z'] == Z)]\n",
    "\n",
    "print(\"Nucleus: A =\",int(A),\" Z =\",int(Z))\n",
    "\n",
    "A = PES_sub['A']\n",
    "Z = PES_sub['Z']\n",
    "Q20 = PES_sub['Q20']\n",
    "Q30 = PES_sub['Q30']\n",
    "E = PES_sub['HFB_cubic']\n",
    "\n",
    "xx = (A.to_numpy(),Z.to_numpy(),Q20.to_numpy(),Q30.to_numpy())\n",
    "xx = np.asarray(xx)\n",
    "xx = xx.T\n",
    "\n",
    "xs_full = xscaler.transform(xx)\n",
    "\n",
    "yy = np.asarray(E).reshape(-1, 1)\n",
    "\n",
    "#model.evaluate(xs_test, ys_test, verbose=1)\n",
    "pes=model.predict(xs_full)\n",
    "\n",
    "# Shift back for plotting\n",
    "\n",
    "x_full = xscaler.inverse_transform(xs_full)\n",
    "\n",
    "fullen_out = pes.T.squeeze()#yscaler.inverse_transform(pes).T.squeeze()\n",
    "\n",
    "print(\"Chosen nucleus error: \",sum((fullen_out - np.asarray(E))**2)/fullen_out.size)\n",
    "\n",
    "#print(traindiff)\n",
    "traindiff = trainen_test-trainen_out.T.squeeze()\n",
    "\n",
    "t2 = traindiff**2\n",
    "print(\"Training error: \",sum(t2)/traindiff.size)\n",
    "\n",
    "#t2 = diff**2\n",
    "#print(\"Test set error: \",sum(t2)/diff.size)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(20,8), gridspec_kw={'width_ratios': [0.8, 1]})\n",
    "#im = ax.imshow(np.reshape(Q20.to_numpy(),(126,21)), np.reshape(Q30.to_numpy(),(126,21)), np.reshape(E.to_numpy(),(126,21)), interpolation='bilinear', origin='lower')#,cmap=mpl.cm.seismic)\n",
    "\n",
    "# Plot the surface.\n",
    "\n",
    "cmp = 'coolwarm'\n",
    "\n",
    "surf = axs[0].contour(np.reshape(Q20.to_numpy(),(126,21)), np.reshape(Q30.to_numpy(),(126,21)), np.reshape(E.to_numpy(),(126,21)))#, cmap=mpl.cm.seismic)\n",
    "axs[0].set_title(\"From Data\")\n",
    "img = axs[0].imshow(np.reshape(E.to_numpy(),(126,21)).T, extent=[0, 250, 0, 60], origin='lower',\n",
    "           cmap=cmp,interpolation='bilinear',aspect=\"auto\")\n",
    "surf = axs[1].contour(np.reshape(Q20.to_numpy(),(126,21)), np.reshape(Q30.to_numpy(),(126,21)), np.reshape(fullen_out,(126,21)))#, cmap=mpl.cm.seismic)\n",
    "axs[1].set_title(\"From NN\")\n",
    "img = axs[1].imshow(np.reshape(fullen_out,(126,21)).T, extent=[0, 250, 0, 60], origin='lower',\n",
    "           cmap=cmp,interpolation='bilinear',aspect=\"auto\")\n",
    "fig.colorbar(img)\n",
    "\n",
    "# Customize the z axis.\n",
    "#ax.set_zlim(-1.01, 1.01)\n",
    "\n",
    "#ax.zaxis.set_major_formatter('{x:.02f}')\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "#fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "#cbar = fig.colorbar(surf)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PESNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
